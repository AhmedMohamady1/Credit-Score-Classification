{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3685b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atef1\\AppData\\Local\\Temp\\ipykernel_10452\\3022883872.py:19: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('train.csv')\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & load\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from scipy.stats import mode # No longer needed for categorical mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "y = df.pop('Credit_Score')  # target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07deb854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit_Mix\n",
      "Standard    36479\n",
      "Good        24337\n",
      "_           20195\n",
      "Bad         18989\n",
      "Name: count, dtype: int64\n",
      "Occupation\n",
      "_______          7062\n",
      "Lawyer           6575\n",
      "Architect        6355\n",
      "Engineer         6350\n",
      "Scientist        6299\n",
      "Mechanic         6291\n",
      "Accountant       6271\n",
      "Developer        6235\n",
      "Media_Manager    6232\n",
      "Teacher          6215\n",
      "Entrepreneur     6174\n",
      "Doctor           6087\n",
      "Journalist       6085\n",
      "Manager          5973\n",
      "Musician         5911\n",
      "Writer           5885\n",
      "Name: count, dtype: int64\n",
      "Payment_Behaviour\n",
      "Low_spent_Small_value_payments      25513\n",
      "High_spent_Medium_value_payments    17540\n",
      "Low_spent_Medium_value_payments     13861\n",
      "High_spent_Large_value_payments     13721\n",
      "High_spent_Small_value_payments     11340\n",
      "Low_spent_Large_value_payments      10425\n",
      "!@9#%8                               7600\n",
      "Name: count, dtype: int64\n",
      "Month\n",
      "January     12500\n",
      "February    12500\n",
      "March       12500\n",
      "April       12500\n",
      "May         12500\n",
      "June        12500\n",
      "July        12500\n",
      "August      12500\n",
      "Name: count, dtype: int64\n",
      "Payment_of_Min_Amount\n",
      "Yes    52326\n",
      "No     35667\n",
      "NM     12007\n",
      "Name: count, dtype: int64\n",
      "Type_of_Loan\n",
      "Not Specified                                                                                                                         1408\n",
      "Credit-Builder Loan                                                                                                                   1280\n",
      "Personal Loan                                                                                                                         1272\n",
      "Debt Consolidation Loan                                                                                                               1264\n",
      "Student Loan                                                                                                                          1240\n",
      "                                                                                                                                      ... \n",
      "Not Specified, Mortgage Loan, Auto Loan, and Payday Loan                                                                                 8\n",
      "Payday Loan, Mortgage Loan, Debt Consolidation Loan, and Student Loan                                                                    8\n",
      "Debt Consolidation Loan, Auto Loan, Personal Loan, Debt Consolidation Loan, Student Loan, and Credit-Builder Loan                        8\n",
      "Student Loan, Auto Loan, Student Loan, Credit-Builder Loan, Home Equity Loan, Debt Consolidation Loan, and Debt Consolidation Loan       8\n",
      "Personal Loan, Auto Loan, Mortgage Loan, Student Loan, and Student Loan                                                                  8\n",
      "Name: count, Length: 6260, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Credit_Mix'].value_counts())\n",
    "ordinal_features = ['Credit_Mix', 'Payment_of_Min_Amount']\n",
    "nominal_features = ['Occupation', 'Payment_Behaviour', 'Month']\n",
    "print(df['Occupation'].value_counts())\n",
    "print(df['Payment_Behaviour'].value_counts())\n",
    "print(df['Month'].value_counts())\n",
    "print(df['Payment_of_Min_Amount'].value_counts())\n",
    "print(df[\"Type_of_Loan\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Custom Transformers ---\n",
    "\n",
    "# Convert Credit History Age String to Months\n",
    "class CreditHistoryAgeToMonths(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='Credit_History_Age', new_column_name='Credit_History_Age_in_months'):\n",
    "        self.column = column\n",
    "        self.new_column_name = new_column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # No fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.column not in X.columns:\n",
    "            # print(f\"Warning: Column '{self.column}' not found in CreditHistoryAgeToMonths.\") # Optional\n",
    "            return X\n",
    "\n",
    "        def to_months(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            try:\n",
    "                s = str(x).strip()\n",
    "                # More flexible regex to capture numbers, allowing for variations\n",
    "                match = re.search(r'(\\d+)\\s*Years?\\s*(?:and)?\\s*(\\d+)?\\s*Months?', s, re.IGNORECASE)\n",
    "                if match:\n",
    "                    years = int(match.group(1)) if match.group(1) else 0\n",
    "                    months = int(match.group(2)) if match.group(2) else 0\n",
    "                    return years * 12 + months\n",
    "                # Handle cases like \"X Years\" without months specified\n",
    "                match_years_only = re.search(r'(\\d+)\\s*Years?', s, re.IGNORECASE)\n",
    "                if match_years_only:\n",
    "                    years = int(match_years_only.group(1))\n",
    "                    return years * 12\n",
    "                # Handle cases like \"Y Months\" without years specified (less common but possible)\n",
    "                match_months_only = re.search(r'(\\d+)\\s*Months?', s, re.IGNORECASE)\n",
    "                if match_months_only:\n",
    "                    months = int(match_months_only.group(1))\n",
    "                    return months\n",
    "\n",
    "            except Exception: # Catch potential errors during conversion\n",
    "                pass # Return NaN if any error occurs\n",
    "            return np.nan # Return NaN if no match or error\n",
    "\n",
    "        # Use .loc for safer assignment\n",
    "        X.loc[:, self.new_column_name] = X[self.column].apply(to_months)\n",
    "        return X\n",
    "\n",
    "\n",
    "# Clean numeric columns: Remove '-' and '_' then convert to numeric\n",
    "class CleanNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.columns:\n",
    "            # Ensure column exists and is not entirely NaN/empty before processing\n",
    "            if c in X.columns and not X[c].dropna().empty:\n",
    "                # Convert to string first to apply string methods, handle potential non-string types\n",
    "                X[c] = (\n",
    "                    X[c].astype(str)\n",
    "                        .str.replace(r'[-_]', '', regex=True)\n",
    "                        .pipe(pd.to_numeric, errors='coerce') # Convert to numeric, invalid parsing becomes NaN\n",
    "                )\n",
    "            elif c in X.columns:\n",
    "                # If column exists but is empty/all NaN, ensure it's numeric type if possible\n",
    "                X[c] = pd.to_numeric(X[c], errors='coerce')\n",
    "            # else: # Optional warning if column not found\n",
    "            #     print(f\"Warning: Column '{c}' not found in CleanNumeric.\")\n",
    "        return X\n",
    "\n",
    "# Forward/backward fill by Customer_ID for static fields\n",
    "class StaticFieldFiller(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, group_col='Customer_ID'):\n",
    "        self.columns = columns\n",
    "        self.group_col = group_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Check if group column exists\n",
    "        if self.group_col not in X.columns:\n",
    "            # print(f\"Warning: Group column '{self.group_col}' not found in StaticFieldFiller. Skipping.\") # Optional\n",
    "            return X\n",
    "\n",
    "        cols_present = [col for col in self.columns if col in X.columns]\n",
    "        if cols_present:\n",
    "            X.loc[:, cols_present] = X.groupby(self.group_col)[cols_present].transform(lambda g: g.ffill().bfill())\n",
    "        # else: # Optional warning if no target columns found\n",
    "        #      print(f\"Warning: No target columns found in StaticFieldFiller.\")\n",
    "        return X\n",
    "\n",
    "# Fix Num_of_Loan using mode + IQR clipping per group\n",
    "class FixLoanCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, loan_col=\"Num_of_Loan\", type_col=\"Type_of_Loan\"):\n",
    "        self.loan_col = loan_col\n",
    "        self.type_col = type_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting needed, purely stateless transformation\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Define the counting function\n",
    "        def count_loans(loan_type):\n",
    "            if pd.isna(loan_type) or loan_type.strip() == '':\n",
    "                return 0\n",
    "            # Use regex to count ',' and 'and'\n",
    "            # Count commas (,) and 'and' as separators\n",
    "            parts = re.split(r',|\\band\\b', loan_type) # \\b for word or space\n",
    "            # Filter out empty parts after split (in case of double commas, etc.)\n",
    "            valid_parts = [p.strip() for p in parts if p.strip()]\n",
    "            return len(valid_parts)\n",
    "\n",
    "        # Apply the counting logic to the specified column\n",
    "        if self.type_col in X.columns:\n",
    "            X[self.loan_col] = X[self.type_col].apply(count_loans)\n",
    "\n",
    "        return X\n",
    "\n",
    "# Clean Category Strings\n",
    "class CategoryCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.columns:\n",
    "            if c in X.columns:\n",
    "                # Ensure column is string type first\n",
    "                X[c] = X[c].astype(str)\n",
    "                # Apply cleaning steps\n",
    "                X[c] = (\n",
    "                    X[c]\n",
    "                        .str.replace(r'[^A-Za-z\\s]', '', regex=True) # Keep only letters and spaces\n",
    "                        .str.strip()\n",
    "                        .str.replace(r'\\s+', '_', regex=True) # Replace spaces with underscore\n",
    "                        .str.lower()\n",
    "                        .replace(r'^_+$', np.nan, regex=True) # Handle cases that become only underscores\n",
    "                        .replace(r'^\\s*$', np.nan, regex=True) # Replace empty/whitespace-only with NaN\n",
    "                        .replace('nan', np.nan) # Replace string 'nan' with NaN\n",
    "                )\n",
    "            # else: # Optional warning if column not found\n",
    "            #     print(f\"Warning: Column '{c}' not found in CategoryCleaner.\")\n",
    "        return X\n",
    "\n",
    "# Impute Categorical using Local Mode (includes rare values)\n",
    "class LocalModeCatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, window=5):\n",
    "        self.columns = columns\n",
    "        self.window = window\n",
    "        # Store global modes as fallback\n",
    "        self.global_modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit global modes for fallback\n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                # Calculate mode on non-NaN values using pandas mode\n",
    "                mode_val = X[col].dropna().mode()\n",
    "                self.global_modes_[col] = mode_val.iat[0] if not mode_val.empty else np.nan\n",
    "            # else: # Optional warning if column not found\n",
    "            #     print(f\"Warning: Column '{col}' not found during fit in LocalModeCatImputer.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        n = len(X)\n",
    "        for col in self.columns:\n",
    "            if col not in X.columns or col not in self.global_modes_:\n",
    "                # print(f\"Warning: Column '{col}' not found or not fitted in LocalModeCatImputer. Skipping.\") # Optional\n",
    "                continue\n",
    "\n",
    "            # Ensure NaNs are consistent (use pd.isna) and handle string 'nan'\n",
    "            vals = X[col].copy().replace('nan', np.nan)\n",
    "\n",
    "            # Identify rare values (consider if freq=1 is too strict)\n",
    "            # Calculate counts on non-NaN values\n",
    "            counts = vals.value_counts(dropna=True)\n",
    "            # Define rare based on a threshold (e.g., count <= 1)\n",
    "            rare = set(counts[counts <= 1].index)\n",
    "\n",
    "            # Find indices needing imputation (NaN or rare)\n",
    "            indices_to_impute = X.index[vals.isna() | vals.isin(rare)]\n",
    "\n",
    "            for i in indices_to_impute:\n",
    "                lo = max(0, i - self.window)\n",
    "                hi = min(n, i + self.window + 1)\n",
    "\n",
    "                # Get window data using .loc to handle potential index gaps\n",
    "                window_indices = X.index[lo:hi].drop(i, errors='ignore')\n",
    "                # Get non-NaN values from the window\n",
    "                w = X.loc[window_indices, col].dropna()\n",
    "\n",
    "                impute_value = self.global_modes_.get(col, np.nan) # Default to global mode\n",
    "\n",
    "                if not w.empty:\n",
    "                    # Calculate mode of the window using pandas mode\n",
    "                    mode_val = w.mode()\n",
    "                    if not mode_val.empty: # Check if mode calculation returned anything\n",
    "                        impute_value = mode_val.iat[0] # Use local mode if available\n",
    "\n",
    "                # Use .loc for assignment\n",
    "                X.loc[i, col] = impute_value\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MixedCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ordinal_features=None, nominal_features=None):\n",
    "        self.ordinal_features = ordinal_features or []\n",
    "        self.nominal_features = nominal_features or []\n",
    "        self.label_encoders_ = {}\n",
    "        self.fill_values_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Fit LabelEncoders on ordinal features\n",
    "        for col in self.ordinal_features:\n",
    "            if col in X.columns:\n",
    "                le = LabelEncoder()\n",
    "                mode_val = X[col].dropna().mode().iloc[0] if not X[col].dropna().empty else 'UNKNOWN'\n",
    "                self.fill_values_[col] = mode_val\n",
    "                le.fit(X[col].dropna().astype(str).unique())\n",
    "                self.label_encoders_[col] = le\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Apply Label Encoding to ordinal features\n",
    "        for col in self.ordinal_features:\n",
    "            if col in X.columns:\n",
    "                le = self.label_encoders_[col]\n",
    "                fill_val = self.fill_values_.get(col, 'UNKNOWN')\n",
    "                filled_col = X[col].fillna(fill_val).astype(str)\n",
    "                known_mask = filled_col.isin(le.classes_)\n",
    "                encoded_col = pd.Series(index=X.index, dtype=float)\n",
    "                encoded_col.loc[known_mask] = le.transform(filled_col[known_mask])\n",
    "                encoded_col.loc[~known_mask] = -1  # For unseen values\n",
    "                X[col] = encoded_col\n",
    "\n",
    "        # Apply One-Hot Encoding to nominal features\n",
    "        if self.nominal_features:\n",
    "            X = pd.get_dummies(X, columns=self.nominal_features, dummy_na=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# Drop Specified Columns\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure columns to drop actually exist\n",
    "        cols_to_drop = [col for col in self.columns if col in X.columns]\n",
    "        return X.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline Definition ---\n",
    "\n",
    "# Define column groups based on pipeline flow\n",
    "initial_numeric_cols = [\n",
    "    \"Age\", \"Annual_Income\", \"Monthly_Inhand_Salary\", \"Num_Bank_Accounts\",\n",
    "    \"Num_Credit_Card\", \"Interest_Rate\", \"Num_of_Loan\", \"Delay_from_due_date\",\n",
    "    \"Num_of_Delayed_Payment\", \"Changed_Credit_Limit\", \"Num_Credit_Inquiries\",\n",
    "    \"Outstanding_Debt\", \"Total_EMI_per_month\", \"Amount_invested_monthly\",\n",
    "    \"Monthly_Balance\"\n",
    "]\n",
    "# Static columns to fill using ffill/bfill per customer\n",
    "# Ensure these are handled appropriately by cleaning/imputation steps afterwards\n",
    "static_cols = [\"Age\",\"Occupation\",\"Annual_Income\",\"Monthly_Inhand_Salary\",\"Num_Bank_Accounts\",\"Num_Credit_Card\",\"Interest_Rate\"]\n",
    "\n",
    "# Categorical columns needing string cleaning\n",
    "cat_clean_cols = ['Credit_Mix','Payment_Behaviour','Occupation','Payment_of_Min_Amount','Month']\n",
    "\n",
    "# Categorical columns to impute using local mode (AFTER cleaning)\n",
    "cat_impute_cols = ['Credit_Mix','Payment_Behaviour','Occupation','Payment_of_Min_Amount','Month']\n",
    "\n",
    "# Categorical columns to mixed encode (AFTER cleaning and categorical imputation)\n",
    "cat_encode_cols = ['Credit_Mix','Payment_Behaviour','Occupation','Payment_of_Min_Amount','Month']\n",
    "\n",
    "# Numeric columns to impute using local mode (AFTER cleaning, filling, and conversion)\n",
    "# Include the newly created numeric credit history column and other numeric columns\n",
    "num_impute_cols = [\n",
    "    \"Age\", \"Annual_Income\", \"Monthly_Inhand_Salary\", \"Num_Bank_Accounts\",\n",
    "    \"Num_Credit_Card\", \"Interest_Rate\", \"Num_of_Loan\", \"Delay_from_due_date\",\n",
    "    \"Num_of_Delayed_Payment\", \"Changed_Credit_Limit\", \"Num_Credit_Inquiries\",\n",
    "    \"Outstanding_Debt\", \"Total_EMI_per_month\", \"Amount_invested_monthly\",\n",
    "    \"Monthly_Balance\", 'Credit_History_Age_in_months' \n",
    "]\n",
    "ordinal_features = ['Credit_Mix', 'Payment_of_Min_Amount']\n",
    "nominal_features = ['Occupation', 'Payment_Behaviour', 'Month']\n",
    "\n",
    "\n",
    "# Columns to finally drop (IDs, original text fields replaced by converted/encoded ones)\n",
    "cols_to_drop = ['Name','Type_of_Loan','ID','SSN', 'Credit_History_Age', 'Customer_ID'] # Drop original history col\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "# Order matters!\n",
    "full_preprocessor = Pipeline([\n",
    "    # 1) Convert Credit History Age string to months (numeric)\n",
    "    ('credit_age_months', CreditHistoryAgeToMonths(column='Credit_History_Age',\n",
    "        new_column_name='Credit_History_Age_in_months')),\n",
    "\n",
    "    # 2) Clean initial numeric columns (removes symbols, coerces to numeric)\n",
    "    ('clean_numeric',     CleanNumeric(columns=initial_numeric_cols)),\n",
    "\n",
    "    # 3) Forward/backward fill static fields per customer\n",
    "    ('fill_static',       StaticFieldFiller(columns=static_cols, group_col='Customer_ID')),\n",
    "\n",
    "    # 4) Fix Num_of_Loan outliers and NaNs using group-wise mode/IQR\n",
    "    ('fix_loan',          FixLoanCount(loan_col=\"Num_of_Loan\", group_col=\"Type_of_Loan\")),\n",
    "\n",
    "    # 5) Clean categorical strings (removes symbols, standardizes format)\n",
    "    ('clean_cats',        CategoryCleaner(columns=cat_clean_cols)),\n",
    "\n",
    "    # 6) Impute categorical NaNs and rare values using local mode (AFTER cleaning)\n",
    "    ('impute_cat_mode',   LocalModeCatImputer(columns=cat_impute_cols)),\n",
    "\n",
    "    # 7) Label encode your cleaned and imputed categoricals\n",
    "    #('label_encode',      LabelEncodeColumns(columns=cat_encode_cols)),\n",
    "    ('encode', MixedCategoricalEncoder(\n",
    "    ordinal_features=['Credit_Mix', 'Payment_of_Min_Amount', 'Month'],\n",
    "    nominal_features=['Occupation', 'Payment_Behaviour']\n",
    "    )),\n",
    "    # 8) Drop original text/ID columns and the original credit history column\n",
    "    ('drop_cols',         DropColumns(columns=cols_to_drop)),\n",
    "\n",
    "    # 9) Impute numeric NaNs and outliers using local mode (AFTER cleaning, conversion, static fill, loan fix)\n",
    "    #('impute_num_mode',   LocalModeNumImputer(columns=num_impute_cols)),\n",
    "\n",
    "    # 10) Fill any remaining NaNs via KNN imputation (cat columns are now numeric after label encoding)\n",
    "    ('knn_impute', KNNImputer(n_neighbors=5, weights='uniform')),\n",
    "\n",
    "    # 11) Scale numeric features\n",
    "    #('scaler',    StandardScaler()),\n",
    "    # ('minmax_scaler', MinMaxScaler()), # Example if you prefer MinMaxScaler\n",
    "\n",
    "    # 12) Apply PCA for dimensionality reduction\n",
    "    #('pca',       PCA(n_components=0.95)), # Keep 95% of variance\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6988d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline fitting...\n",
      "Pipeline fitting finished.\n",
      "\n",
      "Sample data after preprocessing (first 5 rows):\n",
      "[[7.0000000e+00 5.1000000e+01 1.0158348e+05 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 2.3000000e+01 1.0192695e+05 ... 1.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [2.0000000e+00 4.9000000e+01 1.5887112e+05 ... 1.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [1.0000000e+00 4.6000000e+01 3.5032660e+04 ... 0.0000000e+00\n",
      "  1.0000000e+00 0.0000000e+00]\n",
      " [7.0000000e+00 4.2000000e+01 1.2968028e+05 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [7.0000000e+00 5.3000000e+01 1.1285322e+05 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build final model pipeline (including preprocessor and potentially a classifier)\n",
    "# Currently only includes the preprocessor for testing cleaning\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", full_preprocessor),\n",
    "    # Uncomment and add your classifier here when ready, e.g.:\n",
    "    # (\"classifier\", SVC(kernel=\"rbf\", C=1.0, probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42, stratify=y # Added stratify for classification\n",
    ")\n",
    "\n",
    "print(\"Starting pipeline fitting...\")\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitting finished.\")\n",
    "\n",
    "# Optional: Transform the training data and inspect the output\n",
    "print(\"\\nSample data after preprocessing (first 5 rows):\")\n",
    "# Apply the fitted preprocessor to the first 5 rows of the original X_train\n",
    "X_train_processed_sample = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "# Convert the output numpy array back to a DataFrame for easier inspection\n",
    "# Need to get the column names after preprocessing - this can be tricky with custom transformers\n",
    "# For a simple check, we can just print the numpy array or try to infer names if possible\n",
    "# A more robust way is to fit/transform a small sample separately to get column names\n",
    "try:\n",
    "    # Attempt to get feature names after preprocessing (might not work with all custom transformers)\n",
    "    # This is a common challenge with custom transformers that don't implement get_feature_names_out\n",
    "    # For now, we'll just show the numpy array output\n",
    "    print(X_train_processed_sample)\n",
    "except Exception as e:\n",
    "    print(f\"Could not display as DataFrame: {e}\")\n",
    "    print(X_train_processed_sample) # Print the numpy array\n",
    "\n",
    "# print(\"\\nData types after preprocessing:\")\n",
    "# # This will also be tricky without column names.\n",
    "# # You would typically check the dtype of the resulting numpy array or DataFrame\n",
    "# if isinstance(X_train_processed_sample, np.ndarray):\n",
    "#      print(f\"Output is a numpy array with dtype: {X_train_processed_sample.dtype}\")\n",
    "# else:\n",
    "#      print(X_train_processed_sample.info())\n",
    "\n",
    "\n",
    "# # Predict & evaluate (uncomment when classifier is added and fitted)\n",
    "# print(\"Starting prediction...\")\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# print(\"Prediction finished.\")\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# --> KNN <--\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed_sample2 = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "X_train_processed_sample_df = pd.DataFrame(X_train_processed_sample2)\n",
    "\n",
    "# Define the filename for the CSV\n",
    "output_filename = \"X_train_processed_sample.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# index=False prevents pandas from writing the DataFrame index as a column\n",
    "\n",
    "X_train_processed_sample_df.to_csv(output_filename, index=False)\n",
    "# Show number of rows and columns in the output\n",
    "print(\"✅ Transformed Output Info:\")\n",
    "print(f\"Number of rows: {X_train_processed_sample2.shape[0]}\")\n",
    "print(f\"Number of columns (features): {X_train_processed_sample2.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fee9644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed Output Info:\n",
      "Number of rows: 80000\n",
      "Number of columns (features): 43\n"
     ]
    }
   ],
   "source": [
    "X_train_processed_sample_df.to_csv(output_filename, index=False)\n",
    "# Show number of rows and columns in the output\n",
    "print(\"✅ Transformed Output Info:\")\n",
    "print(f\"Number of rows: {X_train_processed_sample2.shape[0]}\")\n",
    "print(f\"Number of columns (features): {X_train_processed_sample2.shape[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
