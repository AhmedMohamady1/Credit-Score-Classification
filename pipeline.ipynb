{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452d4dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atef1\\AppData\\Local\\Temp\\ipykernel_5064\\838809161.py:19: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('train.csv')\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & load\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from scipy.stats import mode # No longer needed for categorical mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "y = df.pop('Credit_Score')  # target variable\n",
    "\n",
    "# --- Custom Transformers ---\n",
    "\n",
    "# Convert Credit History Age String to Months\n",
    "class CreditHistoryAgeToMonths(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='Credit_History_Age', new_column_name='Credit_History_Age_in_months'):\n",
    "        self.column = column\n",
    "        self.new_column_name = new_column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # No fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.column not in X.columns:\n",
    "            # print(f\"Warning: Column '{self.column}' not found in CreditHistoryAgeToMonths.\") # Optional\n",
    "            return X\n",
    "\n",
    "        def to_months(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            try:\n",
    "                s = str(x).strip()\n",
    "                # More flexible regex to capture numbers, allowing for variations\n",
    "                match = re.search(r'(\\d+)\\s*Years?\\s*(?:and)?\\s*(\\d+)?\\s*Months?', s, re.IGNORECASE)\n",
    "                if match:\n",
    "                    years = int(match.group(1)) if match.group(1) else 0\n",
    "                    months = int(match.group(2)) if match.group(2) else 0\n",
    "                    return years * 12 + months\n",
    "                # Handle cases like \"X Years\" without months specified\n",
    "                match_years_only = re.search(r'(\\d+)\\s*Years?', s, re.IGNORECASE)\n",
    "                if match_years_only:\n",
    "                    years = int(match_years_only.group(1))\n",
    "                    return years * 12\n",
    "                # Handle cases like \"Y Months\" without years specified (less common but possible)\n",
    "                match_months_only = re.search(r'(\\d+)\\s*Months?', s, re.IGNORECASE)\n",
    "                if match_months_only:\n",
    "                    months = int(match_months_only.group(1))\n",
    "                    return months\n",
    "\n",
    "            except Exception: # Catch potential errors during conversion\n",
    "                pass # Return NaN if any error occurs\n",
    "            return np.nan # Return NaN if no match or error\n",
    "\n",
    "        # Use .loc for safer assignment\n",
    "        X.loc[:, self.new_column_name] = X[self.column].apply(to_months)\n",
    "        return X\n",
    "\n",
    "\n",
    "# Clean numeric columns: Remove '-' and '_' then convert to numeric\n",
    "class CleanNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.columns:\n",
    "            # Ensure column exists and is not entirely NaN/empty before processing\n",
    "            if c in X.columns and not X[c].dropna().empty:\n",
    "                # Convert to string first to apply string methods, handle potential non-string types\n",
    "                X[c] = (\n",
    "                    X[c].astype(str)\n",
    "                        .str.replace(r'[-_]', '', regex=True)\n",
    "                        .pipe(pd.to_numeric, errors='coerce') # Convert to numeric, invalid parsing becomes NaN\n",
    "                )\n",
    "            elif c in X.columns:\n",
    "                # If column exists but is empty/all NaN, ensure it's numeric type if possible\n",
    "                X[c] = pd.to_numeric(X[c], errors='coerce')\n",
    "            # else: # Optional warning if column not found\n",
    "            #     print(f\"Warning: Column '{c}' not found in CleanNumeric.\")\n",
    "        return X\n",
    "\n",
    "# Forward/backward fill by Customer_ID for static fields\n",
    "class StaticFieldFiller(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, group_col='Customer_ID'):\n",
    "        self.columns = columns\n",
    "        self.group_col = group_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Check if group column exists\n",
    "        if self.group_col not in X.columns:\n",
    "            # print(f\"Warning: Group column '{self.group_col}' not found in StaticFieldFiller. Skipping.\") # Optional\n",
    "            return X\n",
    "\n",
    "        cols_present = [col for col in self.columns if col in X.columns]\n",
    "        if cols_present:\n",
    "            X.loc[:, cols_present] = X.groupby(self.group_col)[cols_present].transform(lambda g: g.ffill().bfill())\n",
    "        # else: # Optional warning if no target columns found\n",
    "        #      print(f\"Warning: No target columns found in StaticFieldFiller.\")\n",
    "        return X\n",
    "\n",
    "# Fix Num_of_Loan using mode + IQR clipping per group\n",
    "class FixLoanOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, loan_col=\"Num_of_Loan\", group_col=\"Type_of_Loan\"):\n",
    "        self.loan_col = loan_col\n",
    "        self.group_col = group_col\n",
    "        # Store maps after fitting\n",
    "        self.mode_map_ = None\n",
    "        self.low_map_ = None\n",
    "        self.high_map_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate maps during fit\n",
    "        if self.loan_col in X.columns and self.group_col in X.columns:\n",
    "            # Ensure loan_col is numeric before calculations\n",
    "            s_numeric = pd.to_numeric(X[self.loan_col], errors='coerce')\n",
    "            tmp = X[self.group_col].fillna(\"__MISSING__\") # Handle NaNs in group column\n",
    "\n",
    "            # Calculate mode, Q1, Q3 per group\n",
    "            # Use .agg with a lambda that handles empty groups\n",
    "            self.mode_map_  = s_numeric.groupby(tmp).agg(lambda grp: grp.mode().iat[0] if not grp.mode().empty else np.nan)\n",
    "            q1_map          = s_numeric.groupby(tmp).quantile(0.25)\n",
    "            q3_map          = s_numeric.groupby(tmp).quantile(0.75)\n",
    "\n",
    "            # Calculate IQR bounds, handle potential NaNs from quantiles\n",
    "            iqr_map = q3_map - q1_map\n",
    "            self.low_map_   = q1_map - 1.5 * iqr_map\n",
    "            self.high_map_  = q3_map + 1.5 * iqr_map\n",
    "\n",
    "        # else: # Optional warning if columns not found\n",
    "        #     print(f\"Warning: Columns '{self.loan_col}' or '{self.group_col}' not found during fit in FixLoanOutliers.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Check if fit was successful and columns exist\n",
    "        if self.mode_map_ is None or self.loan_col not in X.columns or self.group_col not in X.columns:\n",
    "            # print(f\"Warning: FixLoanOutliers transform skipped - columns missing or fit failed.\") # Optional\n",
    "            return X # Return original X if columns are missing or fit failed\n",
    "\n",
    "        tmp = X[self.group_col].fillna(\"__MISSING__\")\n",
    "        s = pd.to_numeric(X[self.loan_col], errors='coerce') # Ensure numeric for transform too\n",
    "\n",
    "        def cap_fill(grp):\n",
    "            name = grp.name\n",
    "            # Get fitted values, default to global values or NaN if group not seen\n",
    "            m = self.mode_map_.get(name, self.mode_map_.get(\"__MISSING__\", np.nan)) # Fallback to __MISSING__ group mode or NaN\n",
    "            l = self.low_map_.get(name, -np.inf)\n",
    "            h = self.high_map_.get(name, np.inf)\n",
    "\n",
    "            # Fill NaNs with the mode *before* clipping\n",
    "            grp_filled = grp.fillna(m)\n",
    "            # Clip values\n",
    "            grp_clipped = grp_filled.clip(lower=l, upper=h)\n",
    "            # Fill again in case clipping introduced NaNs (unlikely with inf bounds) or original fill value was NaN\n",
    "            grp_final = grp_clipped.fillna(m)\n",
    "\n",
    "            return grp_final\n",
    "\n",
    "        # Apply the cap_fill function to each group\n",
    "        # Use .loc for safer assignment\n",
    "        X.loc[:, self.loan_col] = s.groupby(tmp, group_keys=False).apply(cap_fill)\n",
    "        return X\n",
    "\n",
    "# Clean Category Strings\n",
    "class CategoryCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.columns:\n",
    "            if c in X.columns:\n",
    "                # Ensure column is string type first\n",
    "                X[c] = X[c].astype(str)\n",
    "                # Apply cleaning steps\n",
    "                X[c] = (\n",
    "                    X[c]\n",
    "                        .str.replace(r'[^A-Za-z\\s]', '', regex=True) # Keep only letters and spaces\n",
    "                        .str.strip()\n",
    "                        .str.replace(r'\\s+', '_', regex=True) # Replace spaces with underscore\n",
    "                        .str.lower()\n",
    "                        .replace(r'^_+$', np.nan, regex=True) # Handle cases that become only underscores\n",
    "                        .replace(r'^\\s*$', np.nan, regex=True) # Replace empty/whitespace-only with NaN\n",
    "                        .replace('nan', np.nan) # Replace string 'nan' with NaN\n",
    "                )\n",
    "            # else: # Optional warning if column not found\n",
    "            #     print(f\"Warning: Column '{c}' not found in CategoryCleaner.\")\n",
    "        return X\n",
    "\n",
    "# Impute Categorical using Local Mode (includes rare values)\n",
    "class LocalModeCatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, window=5):\n",
    "        self.columns = columns\n",
    "        self.window = window\n",
    "        # Store global modes as fallback\n",
    "        self.global_modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit global modes for fallback\n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                # Calculate mode on non-NaN values using pandas mode\n",
    "                mode_val = X[col].dropna().mode()\n",
    "                self.global_modes_[col] = mode_val.iat[0] if not mode_val.empty else np.nan\n",
    "            # else: # Optional warning if column not found\n",
    "            #     print(f\"Warning: Column '{col}' not found during fit in LocalModeCatImputer.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        n = len(X)\n",
    "        for col in self.columns:\n",
    "            if col not in X.columns or col not in self.global_modes_:\n",
    "                # print(f\"Warning: Column '{col}' not found or not fitted in LocalModeCatImputer. Skipping.\") # Optional\n",
    "                continue\n",
    "\n",
    "            # Ensure NaNs are consistent (use pd.isna) and handle string 'nan'\n",
    "            vals = X[col].copy().replace('nan', np.nan)\n",
    "\n",
    "            # Identify rare values (consider if freq=1 is too strict)\n",
    "            # Calculate counts on non-NaN values\n",
    "            counts = vals.value_counts(dropna=True)\n",
    "            # Define rare based on a threshold (e.g., count <= 1)\n",
    "            rare = set(counts[counts <= 1].index)\n",
    "\n",
    "            # Find indices needing imputation (NaN or rare)\n",
    "            indices_to_impute = X.index[vals.isna() | vals.isin(rare)]\n",
    "\n",
    "            for i in indices_to_impute:\n",
    "                lo = max(0, i - self.window)\n",
    "                hi = min(n, i + self.window + 1)\n",
    "\n",
    "                # Get window data using .loc to handle potential index gaps\n",
    "                window_indices = X.index[lo:hi].drop(i, errors='ignore')\n",
    "                # Get non-NaN values from the window\n",
    "                w = X.loc[window_indices, col].dropna()\n",
    "\n",
    "                impute_value = self.global_modes_.get(col, np.nan) # Default to global mode\n",
    "\n",
    "                if not w.empty:\n",
    "                    # Calculate mode of the window using pandas mode\n",
    "                    mode_val = w.mode()\n",
    "                    if not mode_val.empty: # Check if mode calculation returned anything\n",
    "                        impute_value = mode_val.iat[0] # Use local mode if available\n",
    "\n",
    "                # Use .loc for assignment\n",
    "                X.loc[i, col] = impute_value\n",
    "\n",
    "        return X\n",
    "class LabelEncodeColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.encoders_ = {}  # Store fitted encoders\n",
    "        self.mode_map_ = {}  # Store mode per column for NA filling\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                le = LabelEncoder()\n",
    "                # Compute mode for the column to fill NaNs\n",
    "                col_mode = X[col].dropna().mode()\n",
    "                self.mode_map_[col] = str(col_mode.iloc[0]) if not col_mode.empty else 'UNKNOWN'\n",
    "                \n",
    "                # Fit encoder on unique non-NA values (as strings)\n",
    "                unique_vals = X[col].dropna().astype(str).unique()\n",
    "                self.encoders_[col] = le.fit(unique_vals)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X.columns and col in self.encoders_:\n",
    "                le = self.encoders_[col]\n",
    "                # Fill NA with the mode value\n",
    "                fill_val = self.mode_map_.get(col, 'UNKNOWN')\n",
    "                filled_col = X[col].fillna(fill_val).astype(str)\n",
    "\n",
    "                known_mask = filled_col.isin(le.classes_)\n",
    "                encoded_series = pd.Series(index=X.index, dtype=float)\n",
    "\n",
    "                encoded_series.loc[known_mask] = le.transform(filled_col[known_mask])\n",
    "                encoded_series.loc[~known_mask] = -1  # Assign -1 to unseen values\n",
    "\n",
    "                X.loc[:, col] = encoded_series\n",
    "        return X\n",
    "\n",
    "# Impute Numeric using Local Mode (includes outliers)\n",
    "class LocalModeNumImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, window=5):\n",
    "        self.columns = columns\n",
    "        self.window = window\n",
    "        # Store bounds and global modes from fit\n",
    "        self.bounds_ = {}\n",
    "        self.global_modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                # Ensure column is numeric for calculations\n",
    "                s = pd.to_numeric(X[col], errors='coerce')\n",
    "                if not s.dropna().empty:\n",
    "                    q1, q3 = s.quantile([.25, .75])\n",
    "                    iqr = q3 - q1\n",
    "                    low, high = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "                    self.bounds_[col] = (low, high)\n",
    "                    # Calculate global mode on non-NaN values using pandas mode\n",
    "                    mode_val = s.dropna().mode()\n",
    "                    self.global_modes_[col] = mode_val.iat[0] if not mode_val.empty else np.nan\n",
    "                else: # Handle empty columns\n",
    "                    self.bounds_[col] = (-np.inf, np.inf)\n",
    "                    self.global_modes_[col] = np.nan\n",
    "            # else: # Optional warning if column not found\n",
    "            #      print(f\"Warning: Column '{col}' not found during fit in LocalModeNumImputer.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        n = len(X)\n",
    "        for col in self.columns:\n",
    "            if col not in X.columns or col not in self.bounds_:\n",
    "                # print(f\"Warning: Column '{col}' not found or not fitted in LocalModeNumImputer. Skipping.\") # Optional\n",
    "                continue\n",
    "\n",
    "            # Ensure column is numeric for processing\n",
    "            s = pd.to_numeric(X[col], errors='coerce').copy()\n",
    "            low, high = self.bounds_[col]\n",
    "\n",
    "            # Find indices needing imputation (NaN or outside bounds)\n",
    "            indices_to_impute = X.index[s.isna() | (s < low) | (s > high)]\n",
    "\n",
    "            for i in indices_to_impute:\n",
    "                lo = max(0, i - self.window)\n",
    "                hi = min(n, i + self.window + 1)\n",
    "\n",
    "                # Get window data using .loc, ensure numeric, and drop NaNs\n",
    "                window_indices = X.index[lo:hi].drop(i, errors='ignore')\n",
    "                w = pd.to_numeric(X.loc[window_indices, col], errors='coerce').dropna()\n",
    "\n",
    "                impute_value = self.global_modes_.get(col, np.nan) # Default to global mode\n",
    "\n",
    "                if not w.empty:\n",
    "                    # Calculate mode of the window using pandas mode\n",
    "                    mode_val = w.mode()\n",
    "                    if not mode_val.empty: # Check if mode calculation returned anything\n",
    "                        impute_value = mode_val.iat[0] # Use local mode if available\n",
    "\n",
    "                # Use .loc for assignment\n",
    "                X.loc[i, col] = impute_value\n",
    "\n",
    "            # Final check: Ensure column remains numeric after imputation\n",
    "            X.loc[:, col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "        return X\n",
    "\n",
    "# Drop Specified Columns\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure columns to drop actually exist\n",
    "        cols_to_drop = [col for col in self.columns if col in X.columns]\n",
    "        return X.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d81cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline Definition ---\n",
    "\n",
    "# Define column groups based on pipeline flow\n",
    "initial_numeric_cols = [\n",
    "    \"Age\", \"Annual_Income\", \"Monthly_Inhand_Salary\", \"Num_Bank_Accounts\",\n",
    "    \"Num_Credit_Card\", \"Interest_Rate\", \"Num_of_Loan\", \"Delay_from_due_date\",\n",
    "    \"Num_of_Delayed_Payment\", \"Changed_Credit_Limit\", \"Num_Credit_Inquiries\",\n",
    "    \"Outstanding_Debt\", \"Total_EMI_per_month\", \"Amount_invested_monthly\",\n",
    "    \"Monthly_Balance\"\n",
    "]\n",
    "# Static columns to fill using ffill/bfill per customer\n",
    "# Ensure these are handled appropriately by cleaning/imputation steps afterwards\n",
    "static_cols = [\"Age\",\"Occupation\",\"Annual_Income\",\"Monthly_Inhand_Salary\",\"Num_Bank_Accounts\",\"Num_Credit_Card\",\"Interest_Rate\"]\n",
    "\n",
    "# Categorical columns needing string cleaning\n",
    "cat_clean_cols = ['Credit_Mix','Payment_Behaviour','Occupation','Payment_of_Min_Amount','Month']\n",
    "\n",
    "# Categorical columns to impute using local mode (AFTER cleaning)\n",
    "cat_impute_cols = ['Credit_Mix','Payment_Behaviour','Occupation','Payment_of_Min_Amount','Month']\n",
    "\n",
    "# Categorical columns to label encode (AFTER cleaning and categorical imputation)\n",
    "cat_encode_cols = ['Credit_Mix','Payment_Behaviour','Occupation','Payment_of_Min_Amount','Month']\n",
    "\n",
    "# Numeric columns to impute using local mode (AFTER cleaning, filling, and conversion)\n",
    "# Include the newly created numeric credit history column and other numeric columns\n",
    "num_impute_cols = [\n",
    "    \"Age\", \"Annual_Income\", \"Monthly_Inhand_Salary\", \"Num_Bank_Accounts\",\n",
    "    \"Num_Credit_Card\", \"Interest_Rate\", \"Num_of_Loan\", \"Delay_from_due_date\",\n",
    "    \"Num_of_Delayed_Payment\", \"Changed_Credit_Limit\", \"Num_Credit_Inquiries\",\n",
    "    \"Outstanding_Debt\", \"Total_EMI_per_month\", \"Amount_invested_monthly\",\n",
    "    \"Monthly_Balance\", 'Credit_History_Age_in_months' \n",
    "]\n",
    "\n",
    "\n",
    "# Columns to finally drop (IDs, original text fields replaced by converted/encoded ones)\n",
    "cols_to_drop = ['Name','Type_of_Loan','ID','SSN', 'Credit_History_Age', 'Customer_ID'] # Drop original history col\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "# Order matters!\n",
    "full_preprocessor = Pipeline([\n",
    "    # 1) Convert Credit History Age string to months (numeric)\n",
    "    ('credit_age_months', CreditHistoryAgeToMonths(column='Credit_History_Age',\n",
    "        new_column_name='Credit_History_Age_in_months')),\n",
    "\n",
    "    # 2) Clean initial numeric columns (removes symbols, coerces to numeric)\n",
    "    ('clean_numeric',     CleanNumeric(columns=initial_numeric_cols)),\n",
    "\n",
    "    # 3) Forward/backward fill static fields per customer\n",
    "    ('fill_static',       StaticFieldFiller(columns=static_cols, group_col='Customer_ID')),\n",
    "\n",
    "    # 4) Fix Num_of_Loan outliers and NaNs using group-wise mode/IQR\n",
    "    ('fix_loan',          FixLoanOutliers(loan_col=\"Num_of_Loan\", group_col=\"Type_of_Loan\")),\n",
    "\n",
    "    # 5) Clean categorical strings (removes symbols, standardizes format)\n",
    "    ('clean_cats',        CategoryCleaner(columns=cat_clean_cols)),\n",
    "\n",
    "    # 6) Impute categorical NaNs and rare values using local mode (AFTER cleaning)\n",
    "    ('impute_cat_mode',   LocalModeCatImputer(columns=cat_impute_cols)),\n",
    "\n",
    "    # 7) Label encode your cleaned and imputed categoricals\n",
    "    ('label_encode',      LabelEncodeColumns(columns=cat_encode_cols)),\n",
    "\n",
    "    # 8) Drop original text/ID columns and the original credit history column\n",
    "    ('drop_cols',         DropColumns(columns=cols_to_drop)),\n",
    "\n",
    "    # 9) Impute numeric NaNs and outliers using local mode (AFTER cleaning, conversion, static fill, loan fix)\n",
    "    ('impute_num_mode',   LocalModeNumImputer(columns=num_impute_cols)),\n",
    "\n",
    "    # 10) Fill any remaining NaNs via KNN imputation (cat columns are now numeric after label encoding)\n",
    "    #('knn_impute', KNNImputer(n_neighbors=5, weights='uniform')),\n",
    "\n",
    "    # 11) Scale numeric features\n",
    "    ('scaler',    StandardScaler()),\n",
    "    # ('minmax_scaler', MinMaxScaler()), # Example if you prefer MinMaxScaler\n",
    "\n",
    "    # 12) Apply PCA for dimensionality reduction\n",
    "    ('pca',       PCA(n_components=0.95)), # Keep 95% of variance\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6988d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline fitting...\n",
      "Pipeline fitting finished.\n",
      "\n",
      "Sample data after preprocessing (first 5 rows):\n",
      "[[-2.20910995  1.80255344 -0.66134142  3.95526806 -0.56567646  0.46555824\n",
      "  -0.22434103  0.9505057   2.96330489 -0.20619482  1.65302874  0.89825971\n",
      "   0.04156496  0.99343532 -1.03906614 -0.77977052 -0.03921545 -0.62293253\n",
      "  -0.29492692]\n",
      " [-2.26943078  1.87313254 -2.18589523  3.22932998  0.42444863 -0.81422706\n",
      "  -1.44240647 -1.39447324  2.66679104 -0.71926089  1.86893154 -0.80914997\n",
      "  -0.27478568  0.19108723  0.55400875 -0.23995233  0.18232405 -0.26125931\n",
      "   0.10326096]\n",
      " [-3.56266589  0.17568498 -0.73253597  3.56917078  0.44913926 -0.77999332\n",
      "  -1.30887888 -0.19836928  3.05490208  0.82699614  2.24919027  0.50982818\n",
      "  -1.05512593  0.69558769 -0.04589288 -0.63268301  1.00900108 -0.11335743\n",
      "   0.56686079]\n",
      " [-0.78490607  0.04470292 -0.70276326  0.30223621 -0.88485732  0.47852289\n",
      "  -0.47758977  1.07628903  0.27775255 -0.40222024 -1.05453236  0.27608401\n",
      "  -0.65288423  0.19156179 -0.54302335 -0.34849873  0.03285309  0.58392736\n",
      "  -0.37941597]\n",
      " [ 3.72081458  0.99312836 -0.92383186  0.12908351 -0.71050439  0.45462794\n",
      "  -0.58781508 -0.47092915  0.08699328 -0.7098813  -0.89974898 -0.38612429\n",
      "  -0.3581011   1.92867875 -0.22914128  0.21014796 -0.23180225  1.41775097\n",
      "  -0.53885358]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build final model pipeline (including preprocessor and potentially a classifier)\n",
    "# Currently only includes the preprocessor for testing cleaning\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", full_preprocessor),\n",
    "    # Uncomment and add your classifier here when ready, e.g.:\n",
    "    # (\"classifier\", SVC(kernel=\"rbf\", C=1.0, probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42, stratify=y # Added stratify for classification\n",
    ")\n",
    "\n",
    "print(\"Starting pipeline fitting...\")\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitting finished.\")\n",
    "\n",
    "# Optional: Transform the training data and inspect the output\n",
    "print(\"\\nSample data after preprocessing (first 5 rows):\")\n",
    "# Apply the fitted preprocessor to the first 5 rows of the original X_train\n",
    "X_train_processed_sample = pipeline.named_steps['preprocessor'].transform(X_train.head())\n",
    "# Convert the output numpy array back to a DataFrame for easier inspection\n",
    "# Need to get the column names after preprocessing - this can be tricky with custom transformers\n",
    "# For a simple check, we can just print the numpy array or try to infer names if possible\n",
    "# A more robust way is to fit/transform a small sample separately to get column names\n",
    "try:\n",
    "    # Attempt to get feature names after preprocessing (might not work with all custom transformers)\n",
    "    # This is a common challenge with custom transformers that don't implement get_feature_names_out\n",
    "    # For now, we'll just show the numpy array output\n",
    "    print(X_train_processed_sample)\n",
    "except Exception as e:\n",
    "    print(f\"Could not display as DataFrame: {e}\")\n",
    "    print(X_train_processed_sample) # Print the numpy array\n",
    "\n",
    "# print(\"\\nData types after preprocessing:\")\n",
    "# # This will also be tricky without column names.\n",
    "# # You would typically check the dtype of the resulting numpy array or DataFrame\n",
    "# if isinstance(X_train_processed_sample, np.ndarray):\n",
    "#      print(f\"Output is a numpy array with dtype: {X_train_processed_sample.dtype}\")\n",
    "# else:\n",
    "#      print(X_train_processed_sample.info())\n",
    "\n",
    "\n",
    "# # Predict & evaluate (uncomment when classifier is added and fitted)\n",
    "# print(\"Starting prediction...\")\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# print(\"Prediction finished.\")\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f4960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed_sample2 = pipeline.named_steps['preprocessor'].transform(X_train.head())\n",
    "\n",
    "X_train_processed_sample_df = pd.DataFrame(X_train_processed_sample2)\n",
    "\n",
    "# Define the filename for the CSV\n",
    "output_filename = \"X_train_processed_sample.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# index=False prevents pandas from writing the DataFrame index as a column\n",
    "X_train_processed_sample_df.to_csv(output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0333870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed Output Info:\n",
      "Number of rows: 80000\n",
      "Number of columns (features): 19\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessor to some data (e.g., the training set)\n",
    "X_train_transformed = pipeline.named_steps[\"preprocessor\"].transform(X_train)\n",
    "\n",
    "# Show number of rows and columns in the output\n",
    "print(\"✅ Transformed Output Info:\")\n",
    "print(f\"Number of rows: {X_train_transformed.shape[0]}\")\n",
    "print(f\"Number of columns (features): {X_train_transformed.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
